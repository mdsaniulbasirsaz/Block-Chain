{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 11:51:33,210 - INFO - Loading and preprocessing dataset...\n",
      "2025-02-08 11:51:33,211 - ERROR - Folder C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\\saz.jpg\\original does not exist.\n",
      "2025-02-08 11:51:33,212 - ERROR - Folder C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\\saz.jpg\\tampered does not exist.\n",
      "2025-02-08 11:51:33,213 - ERROR - No valid images found in the dataset.\n",
      "2025-02-08 11:51:33,213 - ERROR - Dataset loading failed. Please check your dataset.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import hashlib\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class ImageForgeryDetection:\n",
    "    def __init__(self, dataset_dir, img_size=(128, 128)):\n",
    "        \"\"\"\n",
    "        Initialize the ImageForgeryDetection class.\n",
    "        \n",
    "        :param dataset_dir: Path to the dataset directory.\n",
    "        :param img_size: Tuple specifying the size to which images will be resized.\n",
    "        \"\"\"\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.img_size = img_size\n",
    "        self.model = None\n",
    "\n",
    "    def load_and_preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Load and preprocess the dataset.\n",
    "        \n",
    "        :return: Tuple of (train_images, test_images, train_labels, test_labels)\n",
    "        \"\"\"\n",
    "        logging.info(\"Loading and preprocessing dataset...\")\n",
    "\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        # Check if dataset directory exists\n",
    "        if not os.path.exists(self.dataset_dir):\n",
    "            logging.error(f\"Dataset directory {self.dataset_dir} does not exist.\")\n",
    "            return None, None, None, None\n",
    "\n",
    "        # Assuming dataset has two folders: 'original' and 'tampered'\n",
    "        for label in ['original', 'tampered']:\n",
    "            folder_path = os.path.join(self.dataset_dir, label)\n",
    "            if not os.path.exists(folder_path):\n",
    "                logging.error(f\"Folder {folder_path} does not exist.\")\n",
    "                continue\n",
    "\n",
    "            for filename in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    logging.warning(f\"Failed to load image: {img_path}\")\n",
    "                    continue\n",
    "\n",
    "                img = cv2.resize(img, self.img_size)  # Resize image\n",
    "                img = img / 255.0  # Normalize image\n",
    "                images.append(img)\n",
    "                labels.append(1 if label == 'tampered' else 0)  # 0 for original, 1 for tampered\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        images = np.array(images)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Ensure there are enough samples for splitting\n",
    "        if len(images) == 0:\n",
    "            logging.error(\"No valid images found in the dataset.\")\n",
    "            return None, None, None, None\n",
    "\n",
    "        if len(np.unique(labels)) < 2:\n",
    "            logging.error(\"Dataset does not contain enough classes for classification.\")\n",
    "            return None, None, None, None\n",
    "\n",
    "        # Split data into training and testing sets\n",
    "        train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "            images, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Dataset loaded: {len(train_images)} train samples, {len(test_images)} test samples.\")\n",
    "        return train_images, test_images, train_labels, test_labels\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build a Convolutional Neural Network (CNN) model.\n",
    "        \"\"\"\n",
    "        logging.info(\"Building CNN model...\")\n",
    "        self.model = models.Sequential([\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(*self.img_size, 3)),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dropout(0.5),  # Add dropout for regularization\n",
    "            layers.Dense(1, activation='sigmoid')  # Binary classification (original or tampered)\n",
    "        ])\n",
    "\n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        self.model.summary()\n",
    "        logging.info(\"CNN model built successfully.\")\n",
    "\n",
    "    def train_model(self, train_images, train_labels, test_images, test_labels, epochs=10, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the CNN model.\n",
    "        \n",
    "        :param train_images: Training images.\n",
    "        :param train_labels: Training labels.\n",
    "        :param test_images: Testing images.\n",
    "        :param test_labels: Testing labels.\n",
    "        :param epochs: Number of training epochs.\n",
    "        :param batch_size: Batch size for training.\n",
    "        \"\"\"\n",
    "        logging.info(\"Training the model...\")\n",
    "\n",
    "        # Define callbacks for early stopping and model checkpoint\n",
    "        early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "        checkpoint = callbacks.ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "        history = self.model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size,\n",
    "                                 validation_data=(test_images, test_labels),\n",
    "                                 callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "        logging.info(\"Model training completed.\")\n",
    "\n",
    "    def evaluate_model(self, test_images, test_labels):\n",
    "        \"\"\"\n",
    "        Evaluate the trained model on the test dataset.\n",
    "        \n",
    "        :param test_images: Testing images.\n",
    "        :param test_labels: Testing labels.\n",
    "        :return: Tuple of (test_loss, test_accuracy)\n",
    "        \"\"\"\n",
    "        logging.info(\"Evaluating the model...\")\n",
    "        test_loss, test_accuracy = self.model.evaluate(test_images, test_labels)\n",
    "        logging.info(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "        return test_loss, test_accuracy\n",
    "\n",
    "    def generate_image_hash(self, image_path):\n",
    "        \"\"\"\n",
    "        Generate SHA-256 hash of an image file.\n",
    "        \n",
    "        :param image_path: Path to the image file.\n",
    "        :return: SHA-256 hash of the image.\n",
    "        \"\"\"\n",
    "        logging.info(f\"Generating hash for image: {image_path}\")\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            img_bytes = f.read()\n",
    "        return hashlib.sha256(img_bytes).hexdigest()\n",
    "\n",
    "    def verify_image_integrity(self, original_image_path, uploaded_image_path):\n",
    "        \"\"\"\n",
    "        Verify the integrity of an uploaded image using its hash.\n",
    "        \n",
    "        :param original_image_path: Path to the original image.\n",
    "        :param uploaded_image_path: Path to the uploaded image.\n",
    "        :return: Boolean indicating whether the uploaded image matches the original.\n",
    "        \"\"\"\n",
    "        logging.info(\"Verifying image integrity...\")\n",
    "        original_hash = self.generate_image_hash(r\"C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\\saz.jpg\")\n",
    "        uploaded_hash = self.generate_image_hash(r\"C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\\saz - Copy.jpg\")\n",
    "        return original_hash == uploaded_hash\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your dataset directory path\n",
    "    dataset_directory = r\"C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\\saz.jpg\"  \n",
    "\n",
    "    detector = ImageForgeryDetection(dataset_dir=dataset_directory)\n",
    "\n",
    "    # Load and preprocess the dataset\n",
    "    train_images, test_images, train_labels, test_labels = detector.load_and_preprocess_data()\n",
    "\n",
    "    if train_images is not None and test_images is not None:\n",
    "        # Build the CNN model\n",
    "        detector.build_model()\n",
    "\n",
    "        # Train the model\n",
    "        detector.train_model(train_images, train_labels, test_images, test_labels, epochs=10, batch_size=32)\n",
    "\n",
    "        # Evaluate the model\n",
    "        detector.evaluate_model(test_images, test_labels)\n",
    "    else:\n",
    "        logging.error(\"Dataset loading failed. Please check your dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 11:57:18,336 - INFO - Generating hash for image: C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\\saz.jpg\n",
      "2025-02-08 11:57:18,337 - ERROR - Image file not found: C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\\saz1.jpeg\n",
      "2025-02-08 11:57:18,337 - ERROR - One or both images could not be hashed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the uploaded image valid? No\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class ImageIntegrityChecker:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the ImageIntegrityChecker class.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def generate_image_hash(self, image_path):\n",
    "        \"\"\"\n",
    "        Generate SHA-256 hash of an image file.\n",
    "\n",
    "        :param image_path: Path to the image file.\n",
    "        :return: SHA-256 hash of the image.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(image_path):\n",
    "            logging.error(f\"Image file not found: {image_path}\")\n",
    "            return None\n",
    "\n",
    "        logging.info(f\"Generating hash for image: {image_path}\")\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            img_bytes = f.read()\n",
    "        return hashlib.sha256(img_bytes).hexdigest()\n",
    "\n",
    "    def verify_image_integrity(self, original_image_path, uploaded_image_path):\n",
    "        \"\"\"\n",
    "        Verify the integrity of an uploaded image using its hash.\n",
    "\n",
    "        :param original_image_path: Path to the original image.\n",
    "        :param uploaded_image_path: Path to the uploaded image.\n",
    "        :return: Boolean indicating whether the uploaded image matches the original.\n",
    "        \"\"\"\n",
    "        original_hash = self.generate_image_hash(original_image_path)\n",
    "        uploaded_hash = self.generate_image_hash(uploaded_image_path)\n",
    "\n",
    "        if original_hash is None or uploaded_hash is None:\n",
    "            logging.error(\"One or both images could not be hashed.\")\n",
    "            return False\n",
    "\n",
    "        logging.info(f\"Original Image Hash: {original_hash}\")\n",
    "        logging.info(f\"Uploaded Image Hash: {uploaded_hash}\")\n",
    "\n",
    "        if original_hash == uploaded_hash:\n",
    "            logging.info(\"The uploaded image matches the original image.\")\n",
    "            return True\n",
    "        else:\n",
    "            logging.warning(\"The uploaded image does NOT match the original image.\")\n",
    "            return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the ImageIntegrityChecker class\n",
    "    checker = ImageIntegrityChecker()\n",
    "\n",
    "    # Paths to the original and uploaded images\n",
    "    original_image_path = r\"C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\\saz.jpg\"  # Replace with your original image path\n",
    "    uploaded_image_path = r\"C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\\saz1.jpeg\"  # Replace with your uploaded image path\n",
    "\n",
    "    # Verify image integrity\n",
    "    is_valid = checker.verify_image_integrity(original_image_path, uploaded_image_path)\n",
    "\n",
    "    # Print result\n",
    "    print(f\"Is the uploaded image valid? {'Yes' if is_valid else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 12:21:01,314 - INFO - Loading and preprocessing dataset...\n",
      "2025-02-08 12:21:01,716 - INFO - Dataset loaded: 5776 train samples, 1445 test samples.\n",
      "2025-02-08 12:21:01,717 - INFO - Building CNN model...\n",
      "C:\\Users\\saniu\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m3,211,392\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,304,769</span> (12.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,304,769\u001b[0m (12.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,304,769</span> (12.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,304,769\u001b[0m (12.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 12:21:01,805 - INFO - CNN model built successfully.\n",
      "2025-02-08 12:21:01,805 - INFO - Training the model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.9818 - loss: 0.0576"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 12:22:14,825 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 370ms/step - accuracy: 0.9819 - loss: 0.0575 - val_accuracy: 1.0000 - val_loss: 4.0959e-10\n",
      "Epoch 2/10\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 324ms/step - accuracy: 0.9999 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 5.8792e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 319ms/step - accuracy: 0.9998 - loss: 0.0074 - val_accuracy: 1.0000 - val_loss: 1.8772e-07\n",
      "Epoch 4/10\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 320ms/step - accuracy: 0.9997 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 1.2520e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 12:25:21,435 - INFO - Model training completed.\n",
      "2025-02-08 12:25:21,435 - INFO - Evaluating the model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 114ms/step - accuracy: 1.0000 - loss: 2.3219e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 12:25:26,960 - INFO - Test Loss: 0.0000, Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class ImageForgeryDetection:\n",
    "    def __init__(self, dataset_dir, img_size=(128, 128), batch_size=32):\n",
    "        \"\"\"\n",
    "        Initialize the ImageForgeryDetection class.\n",
    "\n",
    "        :param dataset_dir: Path to the dataset directory.\n",
    "        :param img_size: Tuple specifying the size to which images will be resized.\n",
    "        :param batch_size: Batch size for data processing.\n",
    "        \"\"\"\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "\n",
    "    def load_dataset(self):\n",
    "        \"\"\"\n",
    "        Load and preprocess the dataset using tf.data.Dataset API.\n",
    "\n",
    "        :return: Tuple of (train_dataset, test_dataset)\n",
    "        \"\"\"\n",
    "        logging.info(\"Loading and preprocessing dataset...\")\n",
    "\n",
    "        # Define image paths and labels\n",
    "        original_folder = os.path.join(self.dataset_dir, 'original')\n",
    "        tampered_folder = os.path.join(self.dataset_dir, 'tampered')\n",
    "\n",
    "        if not os.path.exists(original_folder) or not os.path.exists(tampered_folder):\n",
    "            logging.error(f\"Original or Tampered folder not found in {self.dataset_dir}.\")\n",
    "            return None, None\n",
    "\n",
    "        original_images = [os.path.join(original_folder, f) for f in os.listdir(original_folder)]\n",
    "        tampered_images = [os.path.join(tampered_folder, f) for f in os.listdir(tampered_folder)]\n",
    "\n",
    "        all_image_paths = original_images + tampered_images\n",
    "        all_labels = [0] * len(original_images) + [1] * len(tampered_images)\n",
    "\n",
    "        # Shuffle the data\n",
    "        combined = list(zip(all_image_paths, all_labels))\n",
    "        np.random.shuffle(combined)\n",
    "        all_image_paths, all_labels = zip(*combined)\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        all_image_paths = np.array(all_image_paths)\n",
    "        all_labels = np.array(all_labels)\n",
    "\n",
    "        # Split into training and testing sets\n",
    "        train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "            all_image_paths, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n",
    "        )\n",
    "\n",
    "        # Define a function to load and preprocess images\n",
    "        def load_and_preprocess_image(image_path, label):\n",
    "            img = tf.io.read_file(image_path)\n",
    "            img = tf.image.decode_jpeg(img, channels=3)\n",
    "            img = tf.image.resize(img, self.img_size)\n",
    "            img = tf.cast(img, tf.float32) / 255.0  # Normalize image\n",
    "            return img, label\n",
    "\n",
    "        # Create tf.data.Dataset pipelines\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "        train_dataset = train_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=1000).batch(self.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
    "        test_dataset = test_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        test_dataset = test_dataset.batch(self.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        logging.info(f\"Dataset loaded: {len(train_paths)} train samples, {len(test_paths)} test samples.\")\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build a Convolutional Neural Network (CNN) model.\n",
    "        \"\"\"\n",
    "        logging.info(\"Building CNN model...\")\n",
    "        self.model = models.Sequential([\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(*self.img_size, 3)),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dropout(0.5),  # Add dropout for regularization\n",
    "            layers.Dense(1, activation='sigmoid')  # Binary classification (original or tampered)\n",
    "        ])\n",
    "\n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        self.model.summary()\n",
    "        logging.info(\"CNN model built successfully.\")\n",
    "\n",
    "    def train_model(self, train_dataset, test_dataset, epochs=10):\n",
    "        \"\"\"\n",
    "        Train the CNN model.\n",
    "\n",
    "        :param train_dataset: Training dataset.\n",
    "        :param test_dataset: Testing dataset.\n",
    "        :param epochs: Number of training epochs.\n",
    "        \"\"\"\n",
    "        logging.info(\"Training the model...\")\n",
    "\n",
    "        # Define callbacks for early stopping and model checkpoint\n",
    "        early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "        checkpoint = callbacks.ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "        history = self.model.fit(train_dataset, epochs=epochs,\n",
    "                                 validation_data=test_dataset,\n",
    "                                 callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "        logging.info(\"Model training completed.\")\n",
    "\n",
    "    def evaluate_model(self, test_dataset):\n",
    "        \"\"\"\n",
    "        Evaluate the trained model on the test dataset.\n",
    "\n",
    "        :param test_dataset: Testing dataset.\n",
    "        :return: Tuple of (test_loss, test_accuracy)\n",
    "        \"\"\"\n",
    "        logging.info(\"Evaluating the model...\")\n",
    "        test_loss, test_accuracy = self.model.evaluate(test_dataset)\n",
    "        logging.info(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "        return test_loss, test_accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your dataset directory path\n",
    "    dataset_directory = r\"C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\"  # Update this path\n",
    "    detector = ImageForgeryDetection(dataset_dir=dataset_directory, img_size=(128, 128), batch_size=32)\n",
    "\n",
    "    # Load the dataset\n",
    "    train_dataset, test_dataset = detector.load_dataset()\n",
    "\n",
    "    if train_dataset is not None and test_dataset is not None:\n",
    "        # Build the CNN model\n",
    "        detector.build_model()\n",
    "\n",
    "        # Train the model\n",
    "        detector.train_model(train_dataset, test_dataset, epochs=10)\n",
    "\n",
    "        # Evaluate the model\n",
    "        detector.evaluate_model(test_dataset)\n",
    "    else:\n",
    "        logging.error(\"Dataset loading failed. Please check your dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 13:06:40,101 - INFO - Loading and preprocessing dataset...\n",
      "2025-02-08 13:06:40,156 - INFO - Dataset loaded: 5778 train samples, 1445 test samples.\n",
      "2025-02-08 13:06:40,158 - INFO - Building CNN model...\n",
      "C:\\Users\\saniu\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_12 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m3,211,392\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,304,769</span> (12.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,304,769\u001b[0m (12.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,304,769</span> (12.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,304,769\u001b[0m (12.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 13:06:40,202 - INFO - CNN model built successfully.\n",
      "2025-02-08 13:06:40,202 - INFO - Training the model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - accuracy: 0.9795 - loss: 0.1060"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 13:07:43,472 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 319ms/step - accuracy: 0.9796 - loss: 0.1057 - val_accuracy: 0.9993 - val_loss: 0.0089\n",
      "Epoch 2/10\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 316ms/step - accuracy: 0.9995 - loss: 0.0260 - val_accuracy: 0.9993 - val_loss: 0.0080\n",
      "Epoch 3/10\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 318ms/step - accuracy: 0.9996 - loss: 0.0112 - val_accuracy: 0.9993 - val_loss: 0.0083\n",
      "Epoch 4/10\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 319ms/step - accuracy: 0.9995 - loss: 0.0058 - val_accuracy: 0.9993 - val_loss: 0.0061\n",
      "Epoch 5/10\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 324ms/step - accuracy: 0.9996 - loss: 0.0040 - val_accuracy: 0.9993 - val_loss: 0.0076\n",
      "Epoch 6/10\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 319ms/step - accuracy: 0.9996 - loss: 0.0167 - val_accuracy: 0.9993 - val_loss: 0.0067\n",
      "Epoch 7/10\n",
      "\u001b[1m181/181\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 319ms/step - accuracy: 0.9998 - loss: 0.0028 - val_accuracy: 0.9993 - val_loss: 0.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 13:13:55,793 - INFO - Model training completed.\n",
      "2025-02-08 13:13:55,794 - INFO - Evaluating the model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.9997 - loss: 0.0029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 13:14:01,227 - INFO - Test Loss: 0.0061, Test Accuracy: 99.93%\n",
      "2025-02-08 13:14:01,230 - INFO - Image added to blockchain: C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\\original\\1 (3).jpg\n",
      "2025-02-08 13:14:01,232 - INFO - Image Hash: f5764ee04c65f80bef06633e53e663d5fbb7fdaa1f4b9ee34212b5f1988b1109\n",
      "2025-02-08 13:14:01,232 - INFO - Hash of 'C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\\tampered\\saz1 copy.jpg': f5764ee04c65f80bef06633e53e663d5fbb7fdaa1f4b9ee34212b5f1988b1109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 13:14:01,380 - INFO - Image 'C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\\tampered\\saz1 copy.jpg' is Original.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import hashlib\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class ImageForgeryDetection:\n",
    "    def __init__(self, dataset_dir, img_size=(128, 128), batch_size=32):\n",
    "        \"\"\"\n",
    "        Initialize the ImageForgeryDetection class.\n",
    "\n",
    "        :param dataset_dir: Path to the dataset directory.\n",
    "        :param img_size: Tuple specifying the size to which images will be resized.\n",
    "        :param batch_size: Batch size for data processing.\n",
    "        \"\"\"\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "\n",
    "    def load_dataset(self):\n",
    "        \"\"\"\n",
    "        Load and preprocess the dataset using tf.data.Dataset API.\n",
    "\n",
    "        :return: Tuple of (train_dataset, test_dataset)\n",
    "        \"\"\"\n",
    "        logging.info(\"Loading and preprocessing dataset...\")\n",
    "\n",
    "        # Define image paths and labels\n",
    "        original_folder = os.path.join(self.dataset_dir, 'original')\n",
    "        tampered_folder = os.path.join(self.dataset_dir, 'tampered')\n",
    "\n",
    "        if not os.path.exists(original_folder) or not os.path.exists(tampered_folder):\n",
    "            logging.error(f\"Original or Tampered folder not found in {self.dataset_dir}.\")\n",
    "            return None, None\n",
    "\n",
    "        original_images = [os.path.join(original_folder, f) for f in os.listdir(original_folder)]\n",
    "        tampered_images = [os.path.join(tampered_folder, f) for f in os.listdir(tampered_folder)]\n",
    "\n",
    "        all_image_paths = original_images + tampered_images\n",
    "        all_labels = [0] * len(original_images) + [1] * len(tampered_images)\n",
    "\n",
    "        # Shuffle the data\n",
    "        combined = list(zip(all_image_paths, all_labels))\n",
    "        np.random.shuffle(combined)\n",
    "        all_image_paths, all_labels = zip(*combined)\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        all_image_paths = np.array(all_image_paths)\n",
    "        all_labels = np.array(all_labels)\n",
    "\n",
    "        # Split into training and testing sets\n",
    "        train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "            all_image_paths, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n",
    "        )\n",
    "\n",
    "        # Define a function to load and preprocess images\n",
    "        def load_and_preprocess_image(image_path, label):\n",
    "            img = tf.io.read_file(image_path)\n",
    "            img = tf.image.decode_jpeg(img, channels=3)\n",
    "            img = tf.image.resize(img, self.img_size)\n",
    "            img = tf.cast(img, tf.float32) / 255.0  # Normalize image\n",
    "            return img, label\n",
    "\n",
    "        # Create tf.data.Dataset pipelines\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "        train_dataset = train_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=1000).batch(self.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
    "        test_dataset = test_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        test_dataset = test_dataset.batch(self.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        logging.info(f\"Dataset loaded: {len(train_paths)} train samples, {len(test_paths)} test samples.\")\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build a Convolutional Neural Network (CNN) model.\n",
    "        \"\"\"\n",
    "        logging.info(\"Building CNN model...\")\n",
    "        self.model = models.Sequential([\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(*self.img_size, 3)),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dropout(0.5),  # Add dropout for regularization\n",
    "            layers.Dense(1, activation='sigmoid')  # Binary classification (original or tampered)\n",
    "        ])\n",
    "\n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        self.model.summary()\n",
    "        logging.info(\"CNN model built successfully.\")\n",
    "\n",
    "    def train_model(self, train_dataset, test_dataset, epochs=10):\n",
    "        \"\"\"\n",
    "        Train the CNN model.\n",
    "\n",
    "        :param train_dataset: Training dataset.\n",
    "        :param test_dataset: Testing dataset.\n",
    "        :param epochs: Number of training epochs.\n",
    "        \"\"\"\n",
    "        logging.info(\"Training the model...\")\n",
    "\n",
    "        # Define callbacks for early stopping and model checkpoint\n",
    "        early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "        checkpoint = callbacks.ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "        history = self.model.fit(train_dataset, epochs=epochs,\n",
    "                                 validation_data=test_dataset,\n",
    "                                 callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "        logging.info(\"Model training completed.\")\n",
    "\n",
    "    def evaluate_model(self, test_dataset):\n",
    "        \"\"\"\n",
    "        Evaluate the trained model on the test dataset.\n",
    "\n",
    "        :param test_dataset: Testing dataset.\n",
    "        :return: Tuple of (test_loss, test_accuracy)\n",
    "        \"\"\"\n",
    "        logging.info(\"Evaluating the model...\")\n",
    "        test_loss, test_accuracy = self.model.evaluate(test_dataset)\n",
    "        logging.info(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "        return test_loss, test_accuracy\n",
    "\n",
    "    def predict_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Predict whether an image is original or tampered.\n",
    "\n",
    "        :param image_path: Path to the image file.\n",
    "        :return: Prediction result (0 for original, 1 for tampered).\n",
    "        \"\"\"\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, self.img_size)\n",
    "        img = tf.cast(img, tf.float32) / 255.0  # Normalize image\n",
    "        img = tf.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "        prediction = self.model.predict(img)\n",
    "        return prediction[0][0]\n",
    "    @staticmethod\n",
    "    def generate_image_hash(image_path):\n",
    "        \"\"\"\n",
    "        Generate SHA-256 hash of an image file.\n",
    "\n",
    "        :param image_path: Path to the image file.\n",
    "        :return: SHA-256 hash of the image.\n",
    "        \"\"\"\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            img_bytes = f.read()\n",
    "        image_hash = hashlib.sha256(img_bytes).hexdigest()\n",
    "        logging.info(f\"Image Hash: {image_hash}\")\n",
    "        return image_hash  \n",
    "\n",
    "class Blockchain:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the Blockchain class.\n",
    "        \"\"\"\n",
    "        self.chain = []\n",
    "        self.create_block(proof=1, previous_hash='0')\n",
    "\n",
    "    def create_block(self, proof, previous_hash):\n",
    "        \"\"\"\n",
    "        Create a new block in the blockchain.\n",
    "\n",
    "        :param proof: Proof of work.\n",
    "        :param previous_hash: Hash of the previous block.\n",
    "        :return: New block.\n",
    "        \"\"\"\n",
    "        block = {\n",
    "            'index': len(self.chain) + 1,\n",
    "            'timestamp': str(time.time()),\n",
    "            'proof': proof,\n",
    "            'previous_hash': previous_hash,\n",
    "            'data': []  # Data field to store image hashes\n",
    "        }\n",
    "        self.chain.append(block)\n",
    "        return block\n",
    "\n",
    "    def get_previous_block(self):\n",
    "        \"\"\"\n",
    "        Get the previous block in the blockchain.\n",
    "\n",
    "        :return: Previous block.\n",
    "        \"\"\"\n",
    "        return self.chain[-1]\n",
    "\n",
    "    def proof_of_work(self, previous_proof):\n",
    "        \"\"\"\n",
    "        Simple proof of work algorithm.\n",
    "\n",
    "        :param previous_proof: Proof of the previous block.\n",
    "        :return: New proof.\n",
    "        \"\"\"\n",
    "        new_proof = 1\n",
    "        check_proof = False\n",
    "        while check_proof is False:\n",
    "            hash_operation = hashlib.sha256(str(new_proof ** 2 - previous_proof ** 2).encode()).hexdigest()\n",
    "            if hash_operation[:4] == '0000':\n",
    "                check_proof = True\n",
    "            else:\n",
    "                new_proof += 1\n",
    "        return new_proof\n",
    "\n",
    "    def hash(self, block):\n",
    "        \"\"\"\n",
    "        Hash a block.\n",
    "\n",
    "        :param block: Block to be hashed.\n",
    "        :return: SHA-256 hash of the block.\n",
    "        \"\"\"\n",
    "        encoded_block = json.dumps(block, sort_keys=True).encode()\n",
    "        return hashlib.sha256(encoded_block).hexdigest()\n",
    "\n",
    "    def add_image_to_blockchain(self, image_path):\n",
    "        \"\"\"\n",
    "        Add an image to the blockchain by storing its hash.\n",
    "\n",
    "        :param image_path: Path to the image file.\n",
    "        \"\"\"\n",
    "        previous_block = self.get_previous_block()\n",
    "        previous_proof = previous_block['proof']\n",
    "        proof = self.proof_of_work(previous_proof)\n",
    "        previous_hash = self.hash(previous_block)\n",
    "\n",
    "        # Generate image hash\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            img_bytes = f.read()\n",
    "        image_hash = hashlib.sha256(img_bytes).hexdigest()\n",
    "\n",
    "        # Add image hash to the block\n",
    "        new_block = self.create_block(proof, previous_hash)\n",
    "        new_block['data'].append({'image_hash': image_hash, 'path': image_path})\n",
    "        logging.info(f\"Image added to blockchain: {image_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your dataset directory path\n",
    "    dataset_directory = r\"C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\"  # Update this path\n",
    "    detector = ImageForgeryDetection(dataset_dir=dataset_directory, img_size=(128, 128), batch_size=32)\n",
    "\n",
    "    # Load the dataset\n",
    "    train_dataset, test_dataset = detector.load_dataset()\n",
    "\n",
    "    if train_dataset is not None and test_dataset is not None:\n",
    "        # Build the CNN model\n",
    "        detector.build_model()\n",
    "\n",
    "        # Train the model\n",
    "        detector.train_model(train_dataset, test_dataset, epochs=10)\n",
    "\n",
    "        # Evaluate the model\n",
    "        detector.evaluate_model(test_dataset)\n",
    "\n",
    "        # Initialize blockchain\n",
    "        blockchain = Blockchain()\n",
    "\n",
    "        # Example: Add an image to the blockchain\n",
    "        example_image_path = r\"C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\\original\\1 (3).jpg\"\n",
    "        blockchain.add_image_to_blockchain(example_image_path)\n",
    "\n",
    "        # Example: Predict if an image is tampered\n",
    "        test_image_path = r\"C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\\tampered\\saz1 copy.jpg\"\n",
    "        image_hash = ImageForgeryDetection.generate_image_hash(test_image_path)\n",
    "        logging.info(f\"Hash of '{test_image_path}': {image_hash}\")\n",
    "        prediction = detector.predict_image(test_image_path)\n",
    "        if prediction < 0.5:\n",
    "            logging.info(f\"Image '{test_image_path}' is Original.\")\n",
    "        else:\n",
    "            logging.info(f\"Image '{test_image_path}' is Tampered.\")\n",
    "    else:\n",
    "        logging.error(\"Dataset loading failed. Please check your dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Image Hash: 8588b218577b0022b1a1fabe4cd9dd04c7951b2de668337355ba6a61538cd9cc\n",
      "Tampered Image Hash: f5764ee04c65f80bef06633e53e663d5fbb7fdaa1f4b9ee34212b5f1988b1109\n",
      "Do the hashes match? No\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def generate_image_hash(image_path):\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        img_bytes = f.read()\n",
    "    return hashlib.sha256(img_bytes).hexdigest()\n",
    "\n",
    "original_image_path = r\"C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\\original\\1 (3).jpg\"\n",
    "tampered_image_path = r\"C:\\Users\\saniu\\Machine Learning 2025\\Block Chain\\tampered\\saz1 copy.jpg\"\n",
    "\n",
    "original_hash = generate_image_hash(original_image_path)\n",
    "tampered_hash = generate_image_hash(tampered_image_path)\n",
    "\n",
    "print(f\"Original Image Hash: {original_hash}\")\n",
    "print(f\"Tampered Image Hash: {tampered_hash}\")\n",
    "print(f\"Do the hashes match? {'Yes' if original_hash == tampered_hash else 'No'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
